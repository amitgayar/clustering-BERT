{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/SSQ/Coursera-UW-Machine-Learning-Clustering-Retrieval/blob/master/Week%206%20PA%201/6_hierarchical_clustering.py\n",
    "\n",
    "# import sframe                           \n",
    "import matplotlib.pyplot as plt       \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.preprocessing import normalize\n",
    "get_ipython().magic(u'matplotlib inline')\n",
    "\n",
    "\n",
    "# wiki = sframe.SFrame('people_wiki.gl/')\n",
    "dataframe=pd.read_csv('tfidf_of_paras.csv')\n",
    "X = dataframe.iloc[:,2:].values\n",
    "X = np.nan_to_num(X)\n",
    "m, feat = X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_sparse_csr(filename):\n",
    "#     loader = np.load(filename)\n",
    "#     data = loader['data']\n",
    "#     indices = loader['indices']\n",
    "#     indptr = loader['indptr']\n",
    "#     shape = loader['shape']\n",
    "#     return csr_matrix( (data, indices, indptr), shape)\n",
    "\n",
    "# tf_idf = load_sparse_csr('people_wiki_tf_idf.npz')\n",
    "# map_index_to_word = sframe.SFrame('people_wiki_map_index_to_word.gl/')\n",
    "tf_idf = X\n",
    "\n",
    "# To be consistent with the k-means assignment, let's normalize all vectors to have unit norm.\n",
    "tf_idf = normalize(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bipartition(cluster, maxiter=400, num_runs=4, seed=None):\n",
    "    data_matrix = cluster['matrix']\n",
    "    dataframe   = cluster['dataframe']\n",
    "    kmeans_model = KMeans(n_clusters=2, max_iter=maxiter, n_init=num_runs, random_state=seed, n_jobs=1)\n",
    "    kmeans_model.fit(data_matrix)\n",
    "    centroids, cluster_assignment = kmeans_model.cluster_centers_, kmeans_model.labels_\n",
    "    data_matrix_left_child, data_matrix_right_child = data_matrix[cluster_assignment==0], data_matrix[cluster_assignment==1]\n",
    "#     cluster_assignment_sa = sframe.SArray(cluster_assignment) ##     AG\n",
    "    cluster_assignment_sa = cluster_assignment                 ##     AG\n",
    "    dataframe_left_child, dataframe_right_child = dataframe[cluster_assignment_sa==0], dataframe[cluster_assignment_sa==1]\n",
    "        \n",
    "    \n",
    "    # Package relevant variables for the child clusters\n",
    "    cluster_left_child  = {'matrix': data_matrix_left_child,\n",
    "                           'dataframe': dataframe_left_child,\n",
    "                           'centroid': centroids[0]}\n",
    "    cluster_right_child = {'matrix': data_matrix_right_child,\n",
    "                           'dataframe': dataframe_right_child,\n",
    "                           'centroid': centroids[1]}\n",
    "    \n",
    "    return (cluster_left_child, cluster_right_child)\n",
    "\n",
    "\n",
    "\n",
    "# Note. For the purpose of the assignment, we set an explicit seed (`seed=1`) to produce identical \n",
    "# outputs for every run. In pratical applications, you might want to use different random seeds for all runs.\n",
    "wiki_data = {'matrix': tf_idf, 'dataframe': dataframe} # no 'centroid' for the root cluster\n",
    "left_child, right_child = bipartition(wiki_data, maxiter=100, num_runs=6, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def display_single_tf_idf_cluster(cluster, map_index_to_word):\n",
    "    wiki_subset   = cluster['dataframe']\n",
    "    tf_idf_subset = cluster['matrix']\n",
    "    centroid      = cluster['centroid']\n",
    "    idx = centroid.argsort()[::-1]\n",
    "    for i in xrange(5):\n",
    "        print('{0:s}:{1:.3f}'.format(map_index_to_word['category'][idx[i]], centroid[idx[i]])),\n",
    "    print('')\n",
    "    distances = pairwise_distances(tf_idf_subset, [centroid], metric='euclidean').flatten()\n",
    "    nearest_neighbors = distances.argsort()\n",
    "    for i in xrange(8):\n",
    "        text = ' '.join(wiki_subset[nearest_neighbors[i]]['text'].split(None, 25)[0:25])\n",
    "        print('* {0:50s} {1:.5f}\\n  {2:s}\\n  {3:s}'.format(wiki_subset[nearest_neighbors[i]]['name'],\n",
    "              distances[nearest_neighbors[i]], text[:90], text[90:180] if len(text) > 90 else ''))\n",
    "    print('')\n",
    "\n",
    "\n",
    "display_single_tf_idf_cluster(left_child, map_index_to_word)\n",
    "display_single_tf_idf_cluster(right_child, map_index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# athletes = left_child\n",
    "# non_athletes = right_child\n",
    "# left_child_athletes, right_child_athletes = bipartition(athletes, maxiter=100, num_runs=6, seed=1)\n",
    "# display_single_tf_idf_cluster(left_child_athletes, map_index_to_word)\n",
    "# display_single_tf_idf_cluster(right_child_athletes, map_index_to_word)\n",
    "# baseball = left_child_athletes\n",
    "# ice_hockey_football = right_child_athletes\n",
    "# left_child_ihs, right_child_ihs = bipartition(ice_hockey_football, maxiter=100, num_runs=6, seed=1)\n",
    "# display_single_tf_idf_cluster(left_child_ihs, map_index_to_word)\n",
    "# display_single_tf_idf_cluster(right_child_ihs, map_index_to_word)\n",
    "# left_child_non_athletes, right_child_non_athletes = bipartition(non_athletes, maxiter=100, num_runs=6, seed=1)\n",
    "# display_single_tf_idf_cluster(left_child_non_athletes, map_index_to_word)\n",
    "# display_single_tf_idf_cluster(right_child_non_athletes, map_index_to_word)\n",
    "# male_non_athletes = left_child_non_athletes\n",
    "# female_non_athletes = right_child_non_athletes\n",
    "# left_child_male_non_athletes, right_child_male_non_athletes = bipartition(male_non_athletes, maxiter=100, num_runs=6, seed=1)\n",
    "# display_single_tf_idf_cluster(left_child_male_non_athletes, map_index_to_word)\n",
    "# display_single_tf_idf_cluster(right_child_male_non_athletes, map_index_to_word)\n",
    "# left_child_female_non_athletes, right_child_female_non_athletes = bipartition(female_non_athletes, maxiter=100, num_runs=6, seed=1)\n",
    "# display_single_tf_idf_cluster(left_child_female_non_athletes, map_index_to_word)\n",
    "# display_single_tf_idf_cluster(right_child_female_non_athletes, map_index_to_word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
