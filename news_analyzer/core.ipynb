{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import errno\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "from torrequest import TorRequest\n",
    "import sys\n",
    "\n",
    "torpassword = 'truenews05101991'\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "from mtranslate import translate\n",
    "from slugify import slugify\n",
    "from goose3 import Goose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ARTICLE_COUNT_LIMIT_PER_KEYWORD': 300,\n",
      " 'LINKS_POST_PROCESSING_CLEAN_HTML_RATIO_LETTERS_LENGTH': 0.33,\n",
      " 'LINKS_POST_PROCESSING_NUM_THREADS': 8,\n",
      " 'RUN_POST_PROCESSING': 1,\n",
      " 'SLEEP_TIME_EVERY_TEN_ARTICLES_IN_SECONDS': 1}\n"
     ]
    }
   ],
   "source": [
    "from constants import *\n",
    "\n",
    "NUMBER_OF_CALLS_TO_GOOGLE_NEWS_ENDPOINT = 0\n",
    "\n",
    "GOOGLE_NEWS_URL = 'https://www.google.co.jp/search?q={}&hl=eng&source=lnt&tbs=cdr%3A1%2Ccd_min%3A{}%2Ccd_max%3A{}&tbm=nws&start={}'\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "def parallel_function(f, sequence, num_threads=None):\n",
    "    from multiprocessing import Pool\n",
    "    pool = Pool(processes=num_threads)\n",
    "    print(f, sequence)\n",
    "    result = pool.map(f, sequence)\n",
    "    cleaned = [x for x in result if x is not None]\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forge_url(q, start, year_start, year_end):\n",
    "    global NUMBER_OF_CALLS_TO_GOOGLE_NEWS_ENDPOINT\n",
    "    NUMBER_OF_CALLS_TO_GOOGLE_NEWS_ENDPOINT += 1\n",
    "    return GOOGLE_NEWS_URL.format(q.replace(' ', '+'), str(year_start), str(year_end), start)\n",
    "\n",
    "\n",
    "def extract_links(content):\n",
    "    soup = BeautifulSoup(content, 'html.parser')  # _sQb top _vQb _mnc\n",
    "    links_list = [(v.attrs['href'], \"\".join([str(x) for x in v.contents]) ) \\\n",
    "\tfor v in soup.find_all('a', {'class': ['l lLrAF', 'RTNUJf']})]\n",
    "    dates_list = [v.text for v in soup.find_all('span', {'class': ['f nsa fwzPFf', 'nsa fwzPFf f']})]\n",
    "    output = []\n",
    "    logging.debug('Link List : {}'.format(str(links_list)))\n",
    "    logging.debug('Date List : {}'.format(str(dates_list)))\n",
    "    for (link, date) in zip(links_list, dates_list):\n",
    "        output.append((link[0], link[1], date))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_news_run(keyword, limit=10, year_start=2010, year_end=2019, debug=True, sleep_time_every_ten_articles=0):\n",
    "    num_articles_index = 0\n",
    "    ua = UserAgent()\n",
    "    result = []\n",
    "    while num_articles_index < limit:\n",
    "        url = forge_url(keyword, num_articles_index, year_start, year_end)\n",
    "        if debug:\n",
    "            logging.debug('For Google -> {}'.format(url))\n",
    "            logging.debug('Total number of calls to Google = {}'.format(NUMBER_OF_CALLS_TO_GOOGLE_NEWS_ENDPOINT))\n",
    "        headers = {'User-Agent': ua.chrome}\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=20)\n",
    "            links = extract_links(response.content)\n",
    "            logging.debug('Extract Links : {}'.format(str(links)))\n",
    "\n",
    "            nb_links = len(links)\n",
    "            if nb_links == 0 and num_articles_index == 0:\n",
    "                \"\"\"raise Exception(\n",
    "                    'No results fetched. Either the keyword is wrong '\n",
    "                    'or you have been banned from Google. Retry tomorrow '\n",
    "                    'or change of IP Address.')\"\"\"\n",
    "                logging.debug('No results')\n",
    "                requests.reset_identity()\n",
    "                logging.debug('IP Changed. Retrying ....')\n",
    "                response = requests.get(url, headers=headers, timeout=20)\n",
    "                links = extract_links(response.content)\n",
    "                logging.debug('{}'.format(links))\n",
    "                nb_links = len(links)\n",
    "                if nb_links == 0 and num_articles_index == 0:\n",
    "                    logging.debug('No Links')\n",
    "\n",
    "            if nb_links == 0:\n",
    "                print('No more news to read for keyword {}.'.format(keyword))\n",
    "                break\n",
    "\n",
    "            for i in range(nb_links):\n",
    "                cur_link = links[i]\n",
    "                logging.debug('Links : {}'.format(str(cur_link)))\n",
    "                logging.debug('TITLE = {}, URL = {}, DATE = {}'.format(cur_link[1], cur_link[0], cur_link[2]))\n",
    "            result.extend(links)\n",
    "        except:\n",
    "            print(sys.exc_info())\n",
    "            logging.debug('Google news TimeOut. Maybe the connection is too slow. Skipping.')\n",
    "            pass\n",
    "        num_articles_index += 10\n",
    "        if debug and sleep_time_every_ten_articles != 0:\n",
    "            logging.debug('Program is going to sleep for {} seconds.'.format(sleep_time_every_ten_articles))\n",
    "        time.sleep(sleep_time_every_ten_articles)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-c8b23a971104>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myear_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2017\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myear_end\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2019\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mARTICLE_COUNT_LIMIT_PER_KEYWORD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mtmp_news_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/{}/news'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mmkdir_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_news_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "def mkdir_p(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exc:\n",
    "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "\n",
    "def run(keyword):\n",
    "        logging.debug('KEYWORD = {}'.format(keyword))\n",
    "        generate_articles(keyword)\n",
    "\n",
    "\n",
    "def generate_articles(keyword, year_start=2017, year_end=2019, limit=data.ARTICLE_COUNT_LIMIT_PER_KEYWORD):\n",
    "    tmp_news_folder = 'data/{}/news'.format(keyword)\n",
    "    mkdir_p(tmp_news_folder)\n",
    "\n",
    "    tmp_link_folder = 'data/{}/links'.format(keyword)\n",
    "    mkdir_p(tmp_link_folder)\n",
    "\n",
    "    pickle_file = '{}/{}_{}_{}_links.pkl'.format(tmp_link_folder, keyword, year_start, year_end)\n",
    "    if os.path.isfile(pickle_file):\n",
    "        logging.debug('Google news links for keyword [{}] have been fetched already.'.format(keyword))\n",
    "        links = pickle.load(open(pickle_file, 'rb'))\n",
    "        logging.debug('Found {} links.'.format(len(links)))\n",
    "    else:\n",
    "        links = google_news_run(keyword=keyword,\n",
    "                                limit=limit,\n",
    "                                year_start=year_start,\n",
    "                                year_end=year_end,\n",
    "                                debug=True,\n",
    "                                sleep_time_every_ten_articles=data.SLEEP_TIME_EVERY_TEN_ARTICLES_IN_SECONDS)\n",
    "        pickle.dump(links, open(pickle_file, 'wb'))\n",
    "    if int(data.RUN_POST_PROCESSING):\n",
    "        retrieve_data_from_links(links, tmp_news_folder)\n",
    "\n",
    "\n",
    "def retrieve_data_for_link(param):\n",
    "    logging.debug('retrieve_data_for_link - param = {}'.format(param))\n",
    "    (full_link, tmp_news_folder) = param\n",
    "    link = full_link[0]\n",
    "    google_title = full_link[1]\n",
    "    link_datetime = full_link[2]\n",
    "    compliant_filename_for_link = slugify(link)[:50]\n",
    "    max_len = 100\n",
    "    if len(compliant_filename_for_link) > max_len:\n",
    "        logging.debug('max length exceeded for filename ({}). Truncating.'.format(compliant_filename_for_link))\n",
    "        compliant_filename_for_link = compliant_filename_for_link[:max_len]\n",
    "    pickle_file = '{}/{}.pkl'.format(tmp_news_folder, compliant_filename_for_link)\n",
    "    already_fetched = os.path.isfile(pickle_file)\n",
    "    if not already_fetched:\n",
    "        try:\n",
    "            \"\"\"html = download_html_from_link(link)\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            content = get_content(soup)\n",
    "            full_title = complete_title(soup, google_title)\n",
    "            \"\"\"\n",
    "            goose_client = Goose()\n",
    "            g_content = goose_client.extract(url = link)\n",
    "            article = {'link': link,\n",
    "                       'title': g_content.title,\n",
    "                       'content': g_content.cleaned_text,\n",
    "                        'meta_description': g_content.meta_description,\n",
    "                       'datetime': link_datetime\n",
    "                       }\n",
    "            pickle.dump(article, open(pickle_file, 'wb'))\n",
    "        except Exception as e:\n",
    "            logging.error(e)\n",
    "            logging.error('ERROR - could not download article with link {}'.format(link))\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
