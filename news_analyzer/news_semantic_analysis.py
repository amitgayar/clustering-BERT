from nltk import ne_chunk, pos_tag, word_tokenize, sent_tokenize
from nltk.sem import rtuple
from nltk.sem.relextract import NE_CLASSES, _expand, tree2semi_rel, _join, list2sym
import re, sys, json
from collections import defaultdict, Counter
from os import listdir
from os.path import isfile, join
import pickle
import spacy
from textpipeliner import PipelineEngine, Context
from textpipeliner.pipes import *


def semi_rel2reldict(pairs, window=5, trace=False):
     """
     Converts the pairs generated by ``tree2semi_rel`` into a 'reldict': a dictionary which
     stores information about the subject and object NEs plus the filler between them.
     Additionally, a left and right context of length =< window are captured (within
     a given input sentence).
     :param pairs: a pair of list(str) and ``Tree``, as generated by
     :param window: a threshold for the number of items to include in the left and right context
     :type window: int
     :return: 'relation' dictionaries whose keys are 'lcon', 'subjclass', 'subjtext', 'subjsym', 'filler', objclass', objtext', 'objsym' and 'rcon'
     :rtype: list(defaultdict)
     """
     result = []
     while len(pairs) >= 2:
         reldict = defaultdict(str)
         reldict['lcon'] = _join(pairs[0][0][-window:])
         reldict['subjclass'] = pairs[0][1].label()
         reldict['subjtext'] = _join(pairs[0][1].leaves())
         reldict['subjsym'] = list2sym(pairs[0][1].leaves())
         reldict['filler'] = _join(pairs[1][0])
         reldict['untagged_filler'] = _join(pairs[1][0], untag=True)
         reldict['objclass'] = pairs[1][1].label()
         reldict['objtext'] = _join(pairs[1][1].leaves())
         reldict['objsym'] = list2sym(pairs[1][1].leaves())
         reldict['rcon'] = []
         if trace:
             print("(%s(%s, %s)" % (reldict['untagged_filler'], reldict['subjclass'], reldict['objclass']))
         result.append(reldict)
         pairs = pairs[1:]
     return result

def extract_rels(subjclass, objclass, doc, corpus='ace', pattern=None, window=10):
    """
    Filter the output of ``semi_rel2reldict`` according to specified NE classes and a filler pattern.
    The parameters ``subjclass`` and ``objclass`` can be used to restrict the
    Named Entities to particular types (any of 'LOCATION', 'ORGANIZATION',
    'PERSON', 'DURATION', 'DATE', 'CARDINAL', 'PERCENT', 'MONEY', 'MEASURE').
    :param subjclass: the class of the subject Named Entity.
    :type subjclass: str
    :param objclass: the class of the object Named Entity.
    :type objclass: str
    :param doc: input document
    :type doc: ieer document or a list of chunk trees
    :param corpus: name of the corpus to take as input; possible values are
        'ieer' and 'conll2002'
    :type corpus: str
    :param pattern: a regular expression for filtering the fillers of
        retrieved triples.
    :type pattern: SRE_Pattern
    :param window: filters out fillers which exceed this threshold
    :type window: int
    :return: see ``mk_reldicts``
    :rtype: list(defaultdict)
    """

    if subjclass and subjclass not in NE_CLASSES[corpus]:
        if _expand(subjclass) in NE_CLASSES[corpus]:
            subjclass = _expand(subjclass)
        else:
            raise ValueError("your value for the subject type has not been recognized: %s" % subjclass)
    if objclass and objclass not in NE_CLASSES[corpus]:
        if _expand(objclass) in NE_CLASSES[corpus]:
            objclass = _expand(objclass)
        else:
            raise ValueError("your value for the object type has not been recognized: %s" % objclass)

    if corpus == 'ace' or corpus == 'conll2002':
        pairs = tree2semi_rel(doc)
    elif corpus == 'ieer':
        pairs = tree2semi_rel(doc.text) + tree2semi_rel(doc.headline)
    else:
        raise ValueError("corpus type not recognized")

    reldicts = semi_rel2reldict(pairs)

    relfilter = lambda x: (x['subjclass'] == subjclass and
                           len(x['filler'].split()) <= window and
                           pattern.match(x['filler']) and
                           x['objclass'] == objclass)

    return list(filter(relfilter, reldicts))

def extract_relations(text):
    sentences = sent_tokenize(text)
    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]
    tagged_sentences = [pos_tag(sentence) for sentence in tokenized_sentences]
    OF = re.compile(r'.*\bof\b.*')
    for i, sent in enumerate(tagged_sentences):
	#print sent
	#sent = map(lambda wrd: (wrd[0].encode('ascii', 'ignore'), wrd[1]) \
	#    if isinstance(wrd[0], unicode) else wrd, sent)
        sent = ne_chunk(sent)
    	#print sent
	rels = extract_rels('PER', 'GPE', sent, corpus='ace', pattern=OF, window=10)
	for rel in rels:
	    print i, rtuple(rel)

entities = []
stories = []
actors = []
coref_child = {}
coref_main = []


class Entities:
    def __init__(self, text):
    	self._id = len(entities) + 1
        self.text = text
        entities.append(self)
        return None

class Actors:

    def __init__(self, text):
		self.entity = Entities(text)
		self._id = len(actors) + 1
		self.link = ''
		self.similar_texts = []
		self.supp_complementary = {}
		self.facts = {}
		actors.append(self)
		return None

    def __str__(self):
		return "%s %s"%(self.entity.text, self.similar_texts)

def get_actor(text):
    for i in actors:
        if i.entity.text in text:
            if not text in i.similar_texts:
		i.similar_texts.append(text)
            return i
    return Actors(text)

class Story:

    def __init__(self):
		self._id = len(stories) + 1
		self.actor = None
		self.subactor = None
		self.subactor_narrative = ""
		self.narrative = ""
		self.main_line = ""
		self.main_line_start = 0
		self.supporting_data = []
		self.child_story = None
		stories.append(self)

    def search(actors, narrative):
		for i in stories:
		    # Check if actors match
		    if len(set(actors).intersection(set(i.actor, i.subactor))):
		        print narrative.lemma_, i.narrative
		        if len(actor_match) and narrative.lemma_ == i.narrative:
			    print "Similar Story"
			    return i
		return None

    def __str__(self):
		print self.supporting_data
		return "\nStory %s :\n %s, %s, %s - %s %s"%(self._id,
			self.actor.entity.text if self.actor else "NA",
			self.narrative,
			"%s %s"%(self.subactor.entity.text if self.subactor else "NA",
			self.subactor_narrative if len(self.subactor_narrative) else ""),
			json.dumps(self.supporting_data),
			"\nLink to Stories : %s"%(self.child_story._id) if self.child_story else "")



def create_story(text, start_index=0, sub_story=0):
	# Generate Noun Chunks
	text_chunks = list(text.noun_chunks)
	# ==========================
        # ===== FIND ACTOR =========
        # ==========================
	actor_list = filter(lambda x: x.start >= start_index and \
		x.root.dep_ in ['nsubj', 'nsubjpass'], text_chunks)
	if not len(actor_list):
	    print "No Actors Found !!"
	    return None
	actor = actor_list[0]
	print "Actor : ", actor
	for i in actor:
		print i.is_stop, i, i.tag_, i.pos_, i.dep_
	start_next = actor.end
	actor_name = " ".join(map(lambda y: y.text, 
		filter(lambda x: not x.is_stop and not x.pos_ == 'DET', actor)))
	# Check if Actor in Corel Main or Child List
	coref_main_actor = filter(lambda x: x.start == actor.start, coref_main)
	if len(coref_main_actor):
	    print "Skip Corel Text"
	    # Consider Text starting at end of corel object
	    print len(coref_main_actor[0])
	    start_next = coref_main_actor[0].end
	elif actor.start in coref_child.keys():
	    actor_name = coref_child[actor.start].text
	    print "Corel Found !", actor_name
	else:
	    actor_name = actor.text
	# Get Actor Object by comparing actor_text
	actor_obj = get_actor(actor_name)
	print "Actor : ", actor_obj
	narrative = ''
	print "Root Head : ", actor.root.head.pos_
	print "Root Head Text : ", actor.root.head.text
	story = Story()
	story.actor = actor_obj
	# ==========================
	# ===== NARRATIVE ==========
	# ==========================
	if actor.root.head.pos_ == 'VERB':
	    narrative = actor.root.head.text
	    # Find if it is not a VBZ or VBG (verb acting as adjective or noun)
	    if actor.root.head.tag_ == 'VBZ':
		print "------- VERB as ADJECTIVE --------"
		# VERB acting as Adjective, end story here with next NOUN with verb
		verb_list = filter(lambda x: x.tag_ == 'NN', actor.root.head.rights)
		for i in verb_list:
		    print i, i.tag_, i.pos_
		if len(verb_list):
		    main_verb = verb_list[0]
		    narrative = "%s %s"%(narrative, main_verb.text)
		    print narrative
		    start_next = main_verb.i + 1
		    story.narrative = narrative
		    create_story(text, start_next)
		    return story
	    if actor.root.head.tag_ == 'VBG':
		print "------- VERB as NOUN --------"
		# Gather all words with head tag in [VB, VBG]
		for head_tag in map(lambda y: [y.text, y.head.tag_], \
			filter(lambda x: x.i > actor.root.head.i, text)):
			print head_tag
			if head_tag[1] in ['VB', 'VBG']:
				narrative += " %s"%head_tag[0]
			else:
				break
	    print "VERB Relation Found - ", narrative
	else:
	    # Replace searching in Depend Parsed Tree for word
	    child_words = [child for child in actor.root.children]
	    print "Child Words : ", child_words
	    narrative = filter(lambda x: x.pos_ == 'VERB', child_words)
	    if len(narrative):
		narrative = narrative[0].text
		print "Found Narrative"
	print "Narrative : ", narrative
	story.narrative = narrative
	# Check if Actor dep with root is nsubjpass - Means own story
	if actor.root.dep_ == 'nsubjpass':
	    print map(lambda x: [x.text, x.pos_] , actor.root.children) 
	    story_narrative = filter(lambda x: x.pos_ == 'VERB', actor.root.children)
	    if len(story_narrative):
		    story.narrative = " ".join(map(lambda x: x.text, story_narrative))
	    print "Self Story"
	    print story_narrative, story.narrative
	    # Story Ends
	    create_story(text, actor.root.head.i)
	    return story
	pobjs = []
	rem_text = filter(lambda x: x.start >= start_next, text_chunks)
	print start_next
	print "Find Object in : ", rem_text
	# ==========================
        # ===== OBJECT (ACTOR) =====
        # ==========================
	for i in rem_text:
	    print i, i.root.dep_, i.root.head.text
	    if i.root.dep_ == 'pobj':
		# Extra data related to Story
		pobjs.append(i.root.head.text)
	    if i.root.dep_ == 'dobj':
	        # Check if Actor in Corel Child List
	        if i.start in coref_child.keys():
            	    subactor_name = coref_child[i.start]
	            print "Subactor Corel Found !", subactor_name
            	else:
            	    subactor_name = i
		    print "Subactor : ", subactor_name
		subactor_name = " ".join(map(lambda y: y.text,
	                filter(lambda x: not x.is_stop and not x.pos_ == 'DET', subactor_name)))
        	subactor_obj = get_actor(subactor_name)
		story.subactor = subactor_obj
	        story.supporting_data.append([text.text, pobjs])
		print i.start, text[i.start: ]
		if len(text[i.start: ]):
		    print "Continue story : ", text[i.start: ]
		    create_story(text, i.start)
		return story
	    if i.root.dep_ in ['nsubj', 'nsubjpass']:
		# New story
		print "New Story : ", text[i.start: ]
		sub_story = create_story(text, i.start)
		story.child_story = sub_story
		return story
	return story
	



class SpacyAnalysis:

    def __init__(self):
		self.text = ''
		self.headline = ''
		self.meta_description = ''
		self.nlp = spacy.load('en_core_web_lg')
		self.coref_nlp = spacy.load('en_coref_lg')
	
    def new_text(self, headline, meta_description, text):
        self.text = text
        self.headline = headline
        self.meta_description = meta_description
        self.doc = self.nlp(unicode(text.encode('ascii', 'ignore')))
        self.meta_doc = self.nlp(unicode(self.meta_description.encode('ascii', 'ignore')))
        self.h_doc = self.nlp(unicode(self.headline.encode('ascii', 'ignore')))
        self.coref_doc = self.coref_nlp(unicode(text.encode('ascii', 'ignore')))
	
    def extract_entities(self):
		doc_ents = list(self.doc.ents)
		# Only Get Proper Noun
		doc_ents = filter(lambda x: not '\n' in x.text, doc_ents)
		doc_ents = filter(lambda x: x.label_ in ['GPE', 'NORP', 'ORG', 'PERSON'], doc_ents)
		print self.coref_doc._.has_coref
		if self.coref_doc._.has_coref:
		    for i in self.coref_doc._.coref_clusters:
			coref_main.append(i.main)
			coref_child.update(**{d.start: i.main for d in i.mentions[1:]})
		print coref_main, coref_child
		for i in coref_main:
		    print i.start, i.text
		for i in coref_child.keys():
		    print i, coref_child[i]
		print self.doc.text
		for sent in self.doc.sents:
		    print "Sentence : %s.\nTokens : \n"%sent
		    for token in sent:
	                print token.text, token.pos_, token.tag_, token.dep_
		    """
		    V-be - Verb Being (is are was had been)
		    LV - Linking Verb (smell taste look feel )
		    ADV/TP - adverbial of time or place (here, at the library)
		    nsubj - Subject
		    pobj - Indirect Object
		    dobj - Direct Object
		    # Rules : 
		    # 1.) Story => nsubj <pobj> dobj | nsubj pobj Root Word (Verb) | + Add ADJ to Narrative if dobj is followed by ADJ
		    # 2.) Sub-Story => nsubj -> nsubj (x dobj)
		    # 3.) Narrative => nsubj (ROOT) (if not VERB then parse dependency)
		    # 4.) Facts (When, Where) => nsubj + V-be + ADV/TP 
		    # 5.) Subjective Complement => nsubj + V-be + ADJ | nsubj + LV + ADJ
		    # 6.) Corelation => nsubj + V-be + nsubj (subjective complement)
		    """
		    create_story(sent)
		for i in stories:
		    print i
		for i in actors:
		    print i
		print stories, actors
		meta_spans = list(self.meta_doc.ents)
		h_spans = list(self.h_doc.ents)
		print "Total Entities in Doc : ", spans
		print list(self.doc.noun_chunks)
		# map(lambda x: x.text, x. self.doc.noun_chunks)
		print "Total Entities in Meta Description : ", meta_spans
		print list(self.meta_doc.noun_chunks)
		print "Total Entities in Header : ", h_spans
		print list(self.h_doc.noun_chunks)
		print "Headline : ", self.headline
		print "Meta-Description : ", self.meta_description
		t_entities = spans + meta_spans + h_spans
		print "Total Entities : ", t_entities
		proper_nouns = [token.text for token in self.doc \
			if token.is_stop != True and token.is_punct != True and token.pos_ == "PROPN"]
		noun_freq = Counter(proper_nouns)
		common_nouns = noun_freq.most_common(5)
		print proper_nouns, noun_freq, common_nouns
		

def extract_relations_spacy(text):
    
    nlp = spacy.load('en_coref_md')
    doc = nlp(text)
    print doc._.has_coref
    print doc._.coref_clusters
    doc._.coref_clusters
    print doc._.coref_clusters[1].mentions
    print doc._.coref_clusters[1].mentions[-1]
    print doc._.coref_clusters[1].mentions[-1]._.coref_cluster.main
    for i in doc._.coref_clusters:
	for j in i.mentions:
		print j, j._.coref_cluster.main
    nlp = spacy.load('en_core_web_sm')
    doc = nlp(text)
    try:
        from spacy import displacy
        displacy.serve(doc, style='dep')
    except:
        print sys.exc_info()
    try:
        displacy.render(doc, style='dep', jupyter=False)
    except:
        print sys.exc_info()
    nlp = spacy.load("en")
    pipes_structure = [SequencePipe([FindTokensPipe("VERB/nsubj/*"),
                                 NamedEntityFilterPipe(),
                                 NamedEntityExtractorPipe()]),
                   FindTokensPipe("VERB"),
                   AnyPipe([SequencePipe([FindTokensPipe("VBD/dobj/NNP"),
                                          AggregatePipe([NamedEntityFilterPipe("GPE"),
                                                NamedEntityFilterPipe("PERSON")]),
                                          NamedEntityExtractorPipe()]),
                            SequencePipe([FindTokensPipe("VBD/**/*/pobj/NNP"),
                                          AggregatePipe([NamedEntityFilterPipe("LOC"),
                                                NamedEntityFilterPipe("PERSON")]),
                                          NamedEntityExtractorPipe()])])]
    doc = nlp(text)
    engine = PipelineEngine(pipes_structure, Context(doc), [0,1,2])
    print engine.process()



def extract_news_relations(keyword):
    news_folder = 'data/{}/news'.format(keyword)
    news_files = [f for f in listdir(news_folder) if isfile(join(news_folder, f))]
    print news_files
    news_relations = {}
    for nfile in news_files:
	print nfile
	fcontent = pickle.load(open('%s/%s'%(news_folder, nfile), 'rb'))
	sc_obj = SpacyAnalysis()
	sc_obj.new_text(fcontent['title'], fcontent['meta_description'], fcontent['content'])
	sc_obj.extract_entities()
	#print extract_relations_spacy(unicode(fcontent['content'].encode('ascii', 'ignore')))
        break
	#print extract_relations_corefs(fcontent['content'])
	#.encode('ascii', 'ignore'))
	#news_relations[nfile] = extract_relations(fcontent['content'].encode('ascii', 'ignore'))
	
	
if __name__ == '__main__':
	keyword = "temp"#raw_input('keyword : ')
	extract_news_relations(keyword)


